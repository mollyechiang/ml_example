---
title: "Machine Learning Day 2"
author: "David Kane"
date: "11/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(fs)
library(tidymodels)
library(yardstick)
library(gt)
library(tidyverse)

```

## Predicting Democratic Votes


```{r download, cache=TRUE}
# Mostly the same code as Tuesday, but cleaned up. Do you understand what every
# line does? If not, ask your partner!

download.file(url = "https://github.com/TheUpshot/2018-live-poll-results/archive/master.zip",
              destfile = "master.zip",
              quiet = TRUE,
              mode = "wb")

unzip("master.zip")

raw_data <- fs::dir_ls("2018-live-poll-results-master/data") %>%
  map_dfr(read_csv, 
          .id = "source", 
          col_types = cols(.default = col_character(),
                           turnout_scale = col_double(),
                           turnout_score = col_double(),
                           w_LV = col_double(),
                           w_RV = col_double(),
                           final_weight = col_double(),
                           timestamp = col_datetime(format = "")
                            ))

file_delete(c("master.zip", "2018-live-poll-results-master"))

```

```{r clean}
# Data is somewhat messy. Some of our functions require that the dependent
# variable be a factor with two levels, rather than a simple 0/1 variable. Many
# of the variables in the original data are missing for thousands of
# observations. I think it is useful to know the state in which the poll was
# conducted and the office for which the candidates are running.

clean <- raw_data %>% 
  mutate(dvote = as.factor(ifelse(response == "Dem", "Yes", "No"))) %>% 
  mutate(gender = ifelse(gender == "Female", "Female", "Male")) %>% 
  
  mutate(party = case_when(partyid %in% c("Democrat", "Republican") ~ partyid,
                           partyid == "Independent (No party)" ~ "Independent",
                           TRUE ~ "Other")) %>% 
  mutate(state = toupper(str_sub(source, 51, 52))) %>% 
  mutate(office = case_when(str_detect(source, pattern = "sen") ~ "SEN",
                            str_detect(source, pattern = "gov") ~ "GOV",
                            TRUE ~ "HSE")) %>% 
  
  # Might add some other variables later, especially age, education, source.
  
  select(dvote, gender, party, state, office)

```

```{r make_a_model}
# Make a model and then add predictions from that model back to the original
# data. We need the predictions and the original data together so that we can
# evaluate how well our model does. We create two sorts of predictions: the raw
# probability, which is just on a 0--1 scale and the pred_dvote with is a two
# level factor variable just like the original dvote.

# All of this is relatively simple because there is only one independent
# variable in the model and the dependent variable only has two levels.

# glm and binomial means logistic regression

model_1 <- glm(data = clean, dvote ~ gender, family = "binomial")

# ^ this gives coeffecient that says being male means a - ~10% chance of voting dem (compared to being female)
# it is ~10 bc the coefficient was ~.4 and for logistic regression divide coef by 4 to get percent  

x_1 <- clean %>% 
  
  # use predict function w/ type respose (so your result is a percent) - to 
  # get a prediction of how likely a male/feamle will be to vote dem (since 
  # we did dvote ~ gender)
  
  mutate(prediction = predict(model_1, type = "response")) %>% 
  
  # prediction column is oercent chance that person will vote dem (based on gender)
  
  mutate(pred_dvote = as.factor(ifelse(prediction > mean(prediction), "Yes", "No")))

  # if prediction is above mean, then pred_dvote is yes, as in you predict 
  # that person will vote democratic

```

Model 1: dvote ~ gender 

```{r measuring_goodness_of_model1}

# metrics() function takes in a dataframe, a truth column (actual values) and estimate column
# (prediction of the actual values)
# then it creates a tibble:
# accuracy means the proportion of the data that is predicted correctly
# kappa is saying how different the predictions are from if we had just predicted randomly - kap adjusts accuracy based on random preduction
  # in this case cap is .1, so we did 10% better than a random guess

metrics(x_1, truth = dvote, estimate = pred_dvote) %>%
  gt() %>%
  tab_header("Model 1 Performance")

```
 
 \
 
 Model 2: dvote ~ gender + party
 
```{r making_a_better_model}

model_2 <- glm(data = clean, dvote ~ gender + party, family = "binomial")

x_2 <- clean %>% 
  mutate(prediction = predict(model_2, type = "response")) %>% 
  mutate(pred_dvote = as.factor(ifelse(prediction > mean(prediction), "Yes", "No")))

```

```{r evaluating_model_two}

metrics(x_2, truth = dvote, estimate = pred_dvote) %>%
  gt() %>%
  tab_header("Model 2 Performance")

```

\

Model 3: dvote ~ gender + party + state + office

```{r model_3}
model_3 <- glm(data = clean, dvote ~ gender + party + state + office, family = "binomial")

x_3 <- clean %>% 
  mutate(prediction = predict(model_3, type = "response")) %>% 
  mutate(pred_dvote = as.factor(ifelse(prediction > mean(prediction), "Yes", "No")))

metrics(x_3, truth = dvote, estimate = pred_dvote) %>%
  gt() %>%
  tab_header("Model 3 Performance")

```








